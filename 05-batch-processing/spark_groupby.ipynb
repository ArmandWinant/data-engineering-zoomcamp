{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "  .builder\\\n",
    "  .master(\"local[*]\")\\\n",
    "  .appName('test')\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('./data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.createOrReplaceTempView('green_trip_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    date_trunc(\"hour\", lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(*) AS row_count\n",
    "  FROM green_trip_data\n",
    "  WHERE lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "  GROUP BY 1, 2\n",
    "  ORDER BY 1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+---------+\n",
      "|               hour|zone|            amount|row_count|\n",
      "+-------------------+----+------------------+---------+\n",
      "|2020-01-01 00:00:00|   7| 769.7299999999997|       45|\n",
      "|2020-01-01 00:00:00|  17|            195.03|        9|\n",
      "|2020-01-01 00:00:00|  18|               7.8|        1|\n",
      "|2020-01-01 00:00:00|  22|              15.8|        1|\n",
      "|2020-01-01 00:00:00|  24|              87.6|        3|\n",
      "|2020-01-01 00:00:00|  25|             531.0|       26|\n",
      "|2020-01-01 00:00:00|  29|              61.3|        1|\n",
      "|2020-01-01 00:00:00|  32| 68.94999999999999|        2|\n",
      "|2020-01-01 00:00:00|  33|317.27000000000004|       11|\n",
      "|2020-01-01 00:00:00|  35|129.95999999999998|        5|\n",
      "|2020-01-01 00:00:00|  36|295.34000000000003|       11|\n",
      "|2020-01-01 00:00:00|  37|175.67000000000002|        6|\n",
      "|2020-01-01 00:00:00|  38| 98.78999999999999|        2|\n",
      "|2020-01-01 00:00:00|  40|168.98000000000002|        8|\n",
      "|2020-01-01 00:00:00|  41|1363.9599999999987|       84|\n",
      "|2020-01-01 00:00:00|  42| 799.7599999999994|       52|\n",
      "|2020-01-01 00:00:00|  43|            107.52|        6|\n",
      "|2020-01-01 00:00:00|  47|              13.3|        1|\n",
      "|2020-01-01 00:00:00|  49|266.76000000000005|       14|\n",
      "|2020-01-01 00:00:00|  51|              17.8|        2|\n",
      "+-------------------+----+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/14 10:51:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/14 10:51:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/14 10:51:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue\\\n",
    "  .repartition(20)\\\n",
    "  .write.parquet('data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('./data/pq/yellow/*/*')\n",
    "df_yellow.createOrReplaceTempView('yellow_trip_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    date_trunc(\"hour\", tpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(*) AS row_count\n",
    "  FROM yellow_trip_data\n",
    "  WHERE tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "  GROUP BY 1, 2\n",
    "  ORDER BY 1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/14 10:52:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/14 10:52:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue\\\n",
    "  .repartition(20)\\\n",
    "  .write.parquet('data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue_tmp = df_green_revenue\\\n",
    "  .withColumnRenamed('amount', 'green_amount')\\\n",
    "  .withColumnRenamed('row_count', 'green_row_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue_tmp = df_yellow_revenue\\\n",
    "  .withColumnRenamed('amount', 'yellow_amount')\\\n",
    "  .withColumnRenamed('row_count', 'yellow_row_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                        (0 + 8) / 12]\r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet('data/report/revenue/total', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
