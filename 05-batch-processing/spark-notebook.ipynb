{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 00:29:02 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 00:29:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"demo\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"sue\", 32),\n",
    "        (\"li\", 3),\n",
    "        (\"bob\", 75),\n",
    "        (\"heo\", 13),\n",
    "    ],\n",
    "    [\"first_name\", \"age\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       sue| 32|\n",
      "|        li|  3|\n",
      "|       bob| 75|\n",
      "|       heo| 13|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"life_stage\",\n",
    "    when(col(\"age\") < 13, \"child\")\n",
    "    .when(col(\"age\").between(13, 19), \"teenager\")\n",
    "    .otherwise(\"adult\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       sue| 32|     adult|\n",
      "|        li|  3|     child|\n",
      "|       bob| 75|     adult|\n",
      "|       heo| 13|  teenager|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       sue| 32|     adult|\n",
      "|       bob| 75|     adult|\n",
      "|       heo| 13|  teenager|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(col(\"life_stage\").isin([\"teenager\", \"adult\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|   30.75|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df1.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|life_stage|avg(age)|\n",
      "+----------+--------+\n",
      "|     adult|    53.5|\n",
      "|     child|     3.0|\n",
      "|  teenager|    13.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"life_stage\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable(\"some_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       heo| 13|  teenager|\n",
      "|       bob| 75|     adult|\n",
      "|       sue| 32|     adult|\n",
      "|        li|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO some_people VALUES ('frank', 4, 'child')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
